# Field-Flow Fractionation (FFF) Project Lab Book
## Project Overview
> references can be done using [^1] and [^1]: at the end of the doc

**Project Title:** Developing Software for Data Processing from Field-Flow Fractionation (FFF) / Asymmetric Flow Field-Flow Fractionation (AF4)

**Project Descripton:** This project involves developing software to process data from a Field-Flow Fractionation (FFF) instrument, which is used to derive the size distributions of particles in liquid suspensions. This coding-intensive project requires experience with Python or a similar programming language to effectively analyse, clean, and visualise experimental data.

The team at NMI currently use Astra8 Software to do data analysis using their AF4 machine. This software acts as a kind of 'black box', in that obtaining fundamental proof of the algorithms / code used to do data treatment is either extremely difficult to obtain, or not possible (realistically). Additionally, the reports generated by Astra often contain too much non-useful information.

Traceability is a foundational cornerstone of nanometrology (and metrology in general), and so, I've been tasked with creating an open source template that will do the required data treatment, while also giving greater flexibility to displaying only the data that is required for a specific kind of experiment or sequence.

The order of treatments done by the team currently using Astra is:
1. Despiking
2. Baseline Selection
3. Peak Definitions
4. Molar Mass & Radius from Light Scattering (LS) ==not using AF4==
5. Hydrodynamic Radius $R_h$ from LS
6. Distribution Analysis

My goal is to write code that will do each of the above, both in that order, and individually. Once I've got together all of the equations and procedures required, I will transfer the code to Stata (provided by MQ), and then port into OriginLabs, which is the software that NMI staff have access to.

**Location & Supervisor:** National Measurement Institute (NMI), Lindfield Laboratory. Supervised by Mar-Dean Du Plessis

## Executive Summary
This lab book documents the development of a Python-based data processing software for the Field-Flow Fractionation (FFF) instrument utilised by the Nanometrology team at the National Measurement Institute (NMI). The FFF instrument is critical for deriving particle size distributions in liquid suspensions, an important aspect in nanoparticle characterisation. The focus of this placement was on Asymmetric Flow Field-Flow Fractionation (AF4), a specialised form of FFF. The software automates data processing tasks such as despiking, baseline correction, peak detection, and calculation of hydrodynamic radius. Calculating molar mass may be too difficult without an RI detector, and I have not done that yet but will attempt to if I have the capacity at the end. Preliminary results indicate significant improvements in efficiency and repeatability, enhancing NMI's capabilities in nanometrology by improving measurement accuracy and reducing manual workload.

## Introduction
Field-Flow Fractionation (FFF) is a widely used technique in nanometrology for separating and analysing particles based on size, shape, and density (Quattrini et al., 2021). The technique is highly effective for characterising nanoparticles in liquid suspensions, providing detailed insights into critical properties such as particle size distribution (Quattrini et al., 2021). Accurate measurements are essential for research in fields such as material science, biotechnology, and pharmaceuticals (Till et al., 2014).

At NMI, the Nanometrology team employs FFF instruments to support research and provide services for nanoparticle characterisation. However, the raw data generated by these instruments requires extensive manual processing, which is both labour-intensive and prone to inconsistencies. This project aims to automate the data processing pipeline using Python to enhance accuracy, repeatability, and efficiency, ultimately adding value to NMI’s capabilities by reducing human error and standardising data analysis.

The specific focus of this project was on Asymmetric Flow Field-Flow Fractionation (AF4), a variant of FFF that uses a unique channel design to separate particles based on size. AF4 is particularly suited for analysing a wide range of particle sizes, from proteins to larger nanoparticles, making it a powerful tool in the field of nanometrology.


## Project Plan
### Aims and Objectives
The primary goal of this project is to develop a robust software solution for processing data generated by the AF4 instrument, enabling accurate determination of particle size distributions in liquid suspensions. The specific objectives include:

* Developing Python-based software for data handling, including despiking, peak selection, baseline correction, and size distribution analysis.
* Automating key data processing tasks to reduce manual workload and standardise procedures.
* Enhancing the efficiency and repeatability of AF4 data analysis.

### Timeline

> This is a brief timeline and the tasks referenced  will be discussed further below.

| Week | Task(s) | Image References |
| ---- | ---- | ---------------- |
|1|Choose a project and familiarise myself with the theory| |
|2|- Lab Safety Induction <br>- Prepare solution of isopropanol for AF4 machine to stay in over the weekend <br>- Read up on relevant sections of MD's thesis| |
|3|- Attend NMI award ceremony (online)| |
|4|- Work on finte tuning code for the project<br>- Attend High Voltage lab tour| |
|5|- Analyse and compare experiment results between the DLS and AF4 machines to validate data<br>- Investigate inconsistencies with UV plot data from AF4<br>- Shadow MD in wet lab coffee riffing experiment (client work)| |
|6|- Problem solving and troubleshooting issues and deviations in MADLS and DLS experiments| |
|7| | |
|8| | |
|9| | |

## High-Level Overview and Background
### Field-Flow Fractionation (FFF) Overview

Field-Flow Fractionation (FFF) is a technique that allows the separation, or fractionation, of particles and macromolecules based on differences in their size, shape, and density. Fractionation in this context refers to the process of separating components of a mixture so that particles of similar properties are grouped together. In FFF, a field is applied perpendicular to the direction of flow, creating a velocity gradient that enables the differentiation of particles.

Field-Flow Fractionation (FFF) is a separation technique that allows the fractionation of particles and macromolecules based on differences in their hydrodynamic sizes. It works by applying a perpendicular field that forces particles towards the accumulation wall, creating a velocity gradient across the channel.

* **Types of FFF:** Sedimentation, Thermal, Flow, Centrifugal and Electrical FFF.
* **FFF Instrument Components:** Channel, flow pump, detector (e.g., Light Scattering, UV Absorbance).
* **Applications:** Protein aggregation analysis, nanoparticle characterisation, and colloid stability studies.

### Asymmetric Flow Field-Flow Fractionation (AF4)

Asymmetric Flow Field-Flow Fractionation (AF4) is a specific type of FFF that utilises an asymmetric channel, where only one wall allows cross-flow. This asymmetry creates a parabolic flow profile that enables highly efficient size-based separation of particles. AF4 is particularly useful for separating and analysing a wide range of particle sizes, from proteins and polymers to larger nanoparticles.

* **AF4 Channel Design:** AF4 uses a semi-permeable membrane on one side of the channel, which allows the cross-flow to drive smaller particles closer to the accumulation wall while larger particles remain in faster-moving regions.
* **Applications:** AF4 is ideal for nanoparticle characterisation, protein aggregation studies, and the analysis of complex colloidal systems.

### Key Concepts

> Maybe use following format

Item
: asdasd

> Do in alphabetical order

**Hydrodynamic Radius $r_h$**
: Represents the effective radius of a particle in suspension as it experiences hydrodynamic drag, which can be used to determine particle size. It is defined using the **Stokes-Einstein equation**

: $$r_h = \frac{k_B T}{6 \pi \eta D}$$

: Where $ k_B $ is the Boltzmann constant, $ T $ is the absolute temperature, $ \eta $ is the viscosity of the medium, and $ D $ is the diffusion coefficient

**Hydrodynamic Diameter $d_h$**
: Twice the hydrodynamic radius ($ d_h = 2 r_h $).

**Radius of Gyration $r_g$**
: A measure of the distribution of a particle’s mass around its center of mass. For larger molecules ($>10 \text{nm}$), it can be determined using multi-angle light scattering (MALS). The relation for $r_g$ in terms of molecular weight $M$ and the scattering intensity $I$ is
: $${R_g}^2 = \frac{I(q)}{q^2}$$

: where $ q $ is the scattering vector.

**Recovery % (from UV Data)**
: Used to quantify the amount of sample that has been successfully eluted and detected. It is calculated using:

: $$\text{Recovery \%} = \left( \frac{\text{Amount detected}}{\text{Initial amount}} \right) \times 100$$

**Burchard Stockmayer Shape Factor**
: A factor that relates the radius of gyration to the hydrodynamic radius and provides insight into the shape of the macromolecule

: $$\text{Shape Factor} = \frac{Rg}{Rh}$$

**Autocorrelation Function $g_2$**
: ==explain==

**Electric Field Autocorrelation Function $g_1$**
: ==explain==

**Exponential Decay $\Gamma$**
: ==explain==

**Scattering Vector $q$**
: ==explain==

**Refractive Index $n$**
: ==explain==

**$dn/dc$**
: ==explain==

**Viscosity  $\eta$**
: ==explain==

**Rayleigh Ratio $R_{std}(\theta)$**
: ==explain==

**Detector Cosntant $K_{detector}(\theta)$**
: ==explain==

**Absorbance  $A$**
: ==explain==

**Molar Extinction Coefficient $\epsilon$**
: ==explain==

**Optical Constant  $K$**
: ==explain==

**Rayleigh Scattering**
: ==explain==

**Form Factor $P(\theta)$**
: ==explain==



* **Peak Detection:** ==maybe move to python section??== Identifies significant features in the scattering data to understand particle distribution behaviour.
> ==What else???==

## Theory and Key Concepts
### Hydrodynamic Radius Theory & Calculation
==add more theory descriptions, like uses?==

The calculation of $r_h$ for this project is done though the following process:
1. Calculate Intensity Autocorrelation Function $g_2(\tau)$:
$$g_2(\tau)= \frac{\langle I(t) \cdot I(t+]tau \rangle}{ \langle I(t) \rangle ^2}$$

Where:
- asd asdjasdhsajdh:
- $I(t)$ is the intensity at time $t$
- $\langle I \rangle$ is the mean intensity, and
- $ \tau$ is the delay time

2. Note Electric Field Autocorrelation Function $g_1 (\tau)$:
$$g_1(\tau)=e^{(-\Gamma \tau)}$$

Where $\Gamma$ is the decay rate.

3. Relate $g_2(\tau)$ to $g_1(\tau)$:

$$g_2(\tau)=1+\beta |g_1(\tau)|^2$$
Where $\beta$ is the experimental coherence factor, which is instrument specific (approximated as 1). ==why?? Expand on this==

4. Exponential Decay Fit, modelling $g_2(\tau)-1$ as:
$$g_2(\tau)-1=Ae^{(-2\Gamma \tau)}$$

Where $A$ is the amplitude

5. Calculate the Scattering Vector $q$ for each detector angle $\theta$:

$$q=\frac{4\pi n}{\lambda} \sin{\left( \frac{\theta}{2}\right)}$$
Where:

- $n$ is the refractive index of the solvent,
- $\lambda$ is the laser wavelength ($\text{nm}$), and
- $\theta$ is the scattering angle in degrees.

6. Calculate the Diffusion Coefficient $D$, using its relationship to $\Gamma$:
$$\Gamma=Dq^2 \Rightarrow D=\frac{\Gamma}{q^2}$$

7. Use the Stokes-Einstein Equation to Calculate the $r_h$:
$$D=\frac{k_B T}{6\pi \eta r_h} \Rightarrow r_h=\frac{k_B T}{6\pi \eta D}$$
Where:
- $k_B$ is the Boltzmann constant,
- $T$ is the absolute temperature, and
- $\eta$ is the viscosity of the solvent.

> Note: different detector angles provide multiple values for q, improving the accuracy of the calculated hydrodynamic radius by reducing uncertainty through averaging

### Molar Mass Theory and Calculation
> This section requires many assumed values, as well as a refractometer for $dn/dc$ values, which is not currently available at NMI. ==discuss ways of doing calculation without RI detector== 

Where a sample with the required known values is used ==which values??==, Molar Mass $(M)$ can be obtained using the following steps:

1. Calibrate Detectors:

    - Using a standard sample with known Rayleigh Ratio $R_{std}(\theta)$ and concentration $c_{std}$.
    - Measure detector voltages $V_{std}(\theta)$ for the standard

2. Calculate the Detector Constant $K_{detector}$:
$$K_{detector}=\frac{R_{std}(\theta)}{V_{std}(\theta)}$$

3. Compute Sample Rayleigh Ratios:
$$R_{std}(\theta)=K_{detector} \cdot V_{sample}(\theta)$$

4. Determine Sample Concentration $c$:
$$A=\epsilon cl \Rightarrow c=\frac{A}{\epsilon l}$$
Where:
- $A$ is the absorbance from the UV data.
- $\epsilon$ is the molar extinction coefficient $(L \cdot \text{mol}^{-1} \cdot \text{cm}^{-1})$, and
- $l$ is the machine cell path length, in $\text{cm}$.

5. Calculate the Optical Constant $K$:
$$K = \frac{4\pi^2 n^2 (dn/dc)^2}{N_A \lambda^4}$$
Where:
- $n$ is the refractive index of the solvent,
- $dn/dc$ is the refractive index increment ($\text{mL}\cdot \text{g}^{-1}$),
- $N_A$ is Avogadro’s number, and
- $\lambda$ is the wavelength of the laser in $\text{cm}$.

6. Use the Rayleigh Equation to Calculate $M$: ==what is small v large (check astra user guide)==
    - For Small Particles (Rayleigh Scattering):
    $$R(\theta)=KcM \Rightarrow M=\frac{R(\theta)}{Kc}$$

    - For Larger Particles, incorporate the form factor $P(θ)$:
$$R(\theta)=KcMP(\theta)$$

Where $P(\theta)=e^{-\frac{(qR_g)^2}{3}}$ is the form factor for spherical particles, and $r_g$ is the radius of gyration

> Note: This calculation assumes isotropic particles, and deviations may occur for non-spherical particles, which could lead to inaccuracies. Care should be taken to validate assumptions about particle shape during analysis.


## Data Analysis and Processing
### Data Types
* **Time Data:** Represents elapsed time during the fractionation process.
* **Raw Light Scattering Data:** Intensity of scattered light recorded as particles are fractionated.
* **UV Absorbance Data:** Absorbance recorded as a function of time to assess particle concentration.
* ==Count rate==

### Python Code Overview
==to be completed once code is finalised==

#### Useful Libraries
I have used the following libraries:
- `pandas` for data manipulation and analysis, particularly with data structures like DataFrames, which are great for handling tabular data.
- `numpy` for numerical computations in Python, providing support for large, multi-dimensional arrays and matrices. The library comes along with a collection of mathematical functions to operate on these arrays.
- `matplotlib.pyplot`, which is a plotting library used for creating static, interactive, and animated visualizations in Python, particularly for 2D plots and graphs.
- `scipy.signal.medfilt`, a function within the SciPy library’s signal processing module, used for applying a median filter to a signal, which is helpful in reducing noise in data.
- =='scipy.signal.find_peaks'== WRITE ABOUT
- ==include the rest==
#### Despiking Algorithm
Despiking is the process of identifying and removing extreme deviations (spikes) in data. Spikes in data are typically due to noise or external anomalies and do not represent the underlying signal. Example can be seen in the raw data plots in the Practical Notebook. There are three different despiking algorithms that I investigated and tested;
1. Smoothing / Rolling Mean Method
2. Z-Score Method
3. Median Filter Method

**Z-Score Method Equation**

A z-score / Standard Score describes a value’s relationship to the mean of a group of values. It indicates how many standard deviations a data point is from the mean of the dataset. The formula for calculating the z-score of a value $x$ is:
$$z=\frac{x-\mu}{\sigma}$$

* $x$ is the value being evaluated.
* $\mu$ is the mean of the dataset.
* $\sigma$ is the standard deviation of the dataset.

Interpretation:

- A z-score of 0 means the data point is exactly at the mean.
- A positive z-score indicates the data point is above the mean.
- A negative z-score indicates the data point is below the mean.
- For example, a z-score of 2 means the data point is 2 standard deviations above the mean, while a z-score of -1.5 means it is 1.5 standard deviations below the mean.

**Description Table**

|Method|Description|Applications|Notes|
|-|-|-|-|
|**Rolling Mean**| A technique where each data point in a time series is replaced by the average of the neighboring points within a defined window. This method smooths the data by reducing the impact of random noise and spikes.|Useful when the goal is to smooth out short-term fluctuations and highlight longer-term trends or cycles in the data. It’s commonly used in any context where gradual trends are more important than short-lived variations.| While Rolling mean is useful for identifying overall trends, it can smooth out significant but brief variations (potentially losing important data features). Additionally, rolling mean is not very reliable at boundaries due to fewer points being available.|
|**Z-Score**| Involves calculating the standard score (Z-score) of each data point, which measures how many standard deviations a point is from the mean. Data points with Z-scores exceeding a certain threshold are considered outliers or spikes and can be replaced or removed.|best suited for situations where outliers are expected to be rare and significantly different from the rest of the data, such as in sensor readings or quality control processes. It’s also useful when the data is approximately normally distributed.|Useful for identifying significant outliers, it assumes that the data is normally distributed, which is not always the case.
|**Median Filter**|A median filter replaces each data point with the median of its neighbors within a specified window. Unlike the rolling mean, the median filter is robust to outliers because the median is less affected by extreme values.|Effective in applications where spikes are frequent but not indicative of true data trends, such as in image processing, environmental sensor data, or any scenario where random, sharp noise needs to be removed without distorting the underlying signal.|Particularly useful to preserve sharp features / edges within the data.

> below markdown python formatting for reference
```python
import numpy as np
# etc etc 
```
#### Peak Selection Algorithm
> same as above

#### Data Processing Techniques
* **Baseline Correction:** Removes baseline noise to provide a more accurate representation of peak data.
* **Peak Integration:** Calculates the area under peaks to estimate size and concentration.

## Experiment and Lab Work
### Assisted

## References and Annotated Bibliography
> # ==work in progress==
> [^1]:
> Fix later

Small Angle X-Ray Scattering (Terril) obtained from https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.diamond.ac.uk/dam/jcr:fb6f93b8-2cd2-4646-b6ae-4a426d85c19b/Scattering%2520101.pdf&ved=2ahUKEwjQ0MvR64KJAxUtsVYBHaScNZ4QFnoECBUQAQ&usg=AOvVaw2q9wDc-eUOsCVlJrGjzlkb

==Guinier/Fournet, chapter 4==
: Relationship between gyration radius and q

Till, U., Gaucher-Delmas, M., Saint-Aguet, P., Hamon, G., Marty, J.-D., Chassenieux, C., Payré, B., Goudounèche, D., Mingotaud, A.-F., & Violleau, F. (2014). Asymmetrical flow field-flow fractionation with multi-angle light scattering and quasi-elastic light scattering for characterization of polymersomes: comparison with classical techniques. Analytical and Bioanalytical Chemistry, 406(30), 7841–7853. https://doi.org/10.1007/s00216-014-7891-8

Quattrini, F., Berrecoso, G., Crecente-Campo, J., & Alonso, M. J. (2021). Asymmetric flow field-flow fractionation as a multifunctional technique for the characterization of polymeric nanocarriers. Drug Delivery and Translational Research, 11(2), 373–395. https://doi.org/10.1007/s13346-021-00918-5
